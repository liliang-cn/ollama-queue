# Library Integration Example

This example shows how to integrate Ollama Queue as a library in your Go application.

## Simple Integration

```go
package main

import (
    "context"
    "fmt"
    "log"
    "time"

    "github.com/liliang-cn/ollama-queue/internal/models"
    "github.com/liliang-cn/ollama-queue/pkg/queue"
)

func main() {
    // Initialize queue manager
    qm, err := queue.NewQueueManagerWithOptions(
        queue.WithOllamaHost("http://localhost:11434"),
        queue.WithMaxWorkers(4),
        queue.WithStoragePath("./library_example_data"),
        queue.WithLogLevel("info"),
    )
    if err != nil {
        log.Fatal("Failed to create queue manager:", err)
    }
    defer qm.Close()

    // Start the queue manager
    ctx := context.Background()
    if err := qm.Start(ctx); err != nil {
        log.Fatal("Failed to start queue manager:", err)
    }

    fmt.Println("ğŸš€ Ollama Queue started successfully!")

    // Example 1: Simple chat task
    fmt.Println("\nğŸ“ Example 1: Simple Chat Task")
    chatTask := queue.NewChatTask("llama2", []models.ChatMessage{
        {Role: "user", Content: "What is the capital of France?"},
    }, queue.WithTaskPriority(models.PriorityHigh))

    taskID, err := qm.SubmitTask(chatTask)
    if err != nil {
        log.Fatal("Failed to submit task:", err)
    }
    fmt.Printf("âœ… Chat task submitted with ID: %s\n", taskID)

    // Example 2: Async processing with callback
    fmt.Println("\nğŸ”„ Example 2: Async Processing")
    asyncTask := queue.NewGenerateTask(
        "codellama", 
        "Write a simple 'Hello, World!' program in Go",
        queue.WithTaskPriority(models.PriorityNormal),
    )

    _, err = qm.SubmitTaskWithCallback(asyncTask, func(result *models.TaskResult) {
        if result.Success {
            fmt.Printf("âœ… Code generation completed successfully!\n")
            fmt.Printf("ğŸ“„ Result: %v\n", result.Data)
        } else {
            fmt.Printf("âŒ Code generation failed: %s\n", result.Error)
        }
    })
    if err != nil {
        log.Fatal("Failed to submit async task:", err)
    }

    // Example 3: Streaming task
    fmt.Println("\nğŸŒŠ Example 3: Streaming Task")
    streamTask := queue.NewChatTask("llama2", []models.ChatMessage{
        {Role: "user", Content: "Tell me a short joke"},
    }, 
        queue.WithChatStreaming(true),
        queue.WithTaskPriority(models.PriorityHigh),
    )

    streamChan, err := qm.SubmitStreamingTask(streamTask)
    if err != nil {
        log.Fatal("Failed to submit streaming task:", err)
    }

    fmt.Print("ğŸ¤– Response: ")
    for chunk := range streamChan {
        if chunk.Error != nil {
            fmt.Printf("\nâŒ Stream error: %v\n", chunk.Error)
            break
        }
        if chunk.Done {
            fmt.Println("\nâœ… Stream completed!")
            break
        }
        if chunk.Data != "" {
            fmt.Print(chunk.Data)
        }
    }

    // Example 4: Batch processing
    fmt.Println("\nğŸ“¦ Example 4: Batch Processing")
    batchTasks := []*models.Task{
        queue.NewChatTask("llama2", []models.ChatMessage{
            {Role: "user", Content: "What is Go language?"},
        }),
        queue.NewChatTask("llama2", []models.ChatMessage{
            {Role: "user", Content: "What is Python?"},
        }),
        queue.NewEmbedTask("nomic-embed-text", "Sample text for embedding"),
    }

    fmt.Printf("ğŸ“¤ Submitting batch of %d tasks...\n", len(batchTasks))
    results, err := qm.SubmitBatchTasks(batchTasks)
    if err != nil {
        log.Fatal("Failed to submit batch tasks:", err)
    }

    successCount := 0
    for i, result := range results {
        if result.Success {
            successCount++
            fmt.Printf("âœ… Task %d completed successfully\n", i+1)
        } else {
            fmt.Printf("âŒ Task %d failed: %s\n", i+1, result.Error)
        }
    }
    fmt.Printf("ğŸ“Š Batch results: %d/%d tasks successful\n", successCount, len(results))

    // Example 5: Queue monitoring
    fmt.Println("\nğŸ“Š Example 5: Queue Statistics")
    stats, err := qm.GetQueueStats()
    if err != nil {
        log.Fatal("Failed to get queue stats:", err)
    }

    fmt.Printf("ğŸ“ˆ Queue Statistics:\n")
    fmt.Printf("   â€¢ Pending: %d\n", stats.PendingTasks)
    fmt.Printf("   â€¢ Running: %d\n", stats.RunningTasks)
    fmt.Printf("   â€¢ Completed: %d\n", stats.CompletedTasks)
    fmt.Printf("   â€¢ Failed: %d\n", stats.FailedTasks)
    fmt.Printf("   â€¢ Total: %d\n", stats.TotalTasks)
    fmt.Printf("   â€¢ Active Workers: %d\n", stats.WorkersActive)
    fmt.Printf("   â€¢ Idle Workers: %d\n", stats.WorkersIdle)

    // Wait a bit for async tasks to complete
    time.Sleep(5 * time.Second)

    fmt.Println("\nğŸ‰ Library integration example completed!")
}
```

## Running the Example

1. Make sure Ollama is running:
   ```bash
   ollama serve
   ```

2. Download required models:
   ```bash
   ollama pull llama2
   ollama pull codellama
   ollama pull nomic-embed-text
   ```

3. Run the example:
   ```bash
   go run main.go
   ```

## Expected Output

```
ğŸš€ Ollama Queue started successfully!

ğŸ“ Example 1: Simple Chat Task
âœ… Chat task submitted with ID: abc123def456

ğŸ”„ Example 2: Async Processing
âœ… Code generation completed successfully!
ğŸ“„ Result: package main...

ğŸŒŠ Example 3: Streaming Task
ğŸ¤– Response: Here's a joke for you: Why don't scientists trust atoms? Because they make up everything!
âœ… Stream completed!

ğŸ“¦ Example 4: Batch Processing
ğŸ“¤ Submitting batch of 3 tasks...
âœ… Task 1 completed successfully
âœ… Task 2 completed successfully
âœ… Task 3 completed successfully
ğŸ“Š Batch results: 3/3 tasks successful

ğŸ“Š Example 5: Queue Statistics
ğŸ“ˆ Queue Statistics:
   â€¢ Pending: 0
   â€¢ Running: 0
   â€¢ Completed: 7
   â€¢ Failed: 0
   â€¢ Total: 7
   â€¢ Active Workers: 0
   â€¢ Idle Workers: 4

ğŸ‰ Library integration example completed!
```

This example demonstrates all the major features of the Ollama Queue library and can serve as a starting point for your own applications.