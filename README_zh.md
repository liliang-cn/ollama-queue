# Ollama Queue

åŸºäºGoè¯­è¨€æ„å»ºçš„é«˜æ€§èƒ½Ollamaæ¨¡å‹ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†ç³»ç»Ÿã€‚Ollama Queue æä¾›äº†é«˜æ•ˆçš„ä»»åŠ¡è°ƒåº¦ã€ä¼˜å…ˆçº§ç®¡ç†ã€æŒä¹…åŒ–å­˜å‚¨å’Œé‡è¯•æœºåˆ¶ï¼Œä¸“ä¸ºAIæ¨¡å‹æ“ä½œè®¾è®¡ã€‚

**ğŸŒŸ [English](README.md)** | **ğŸ“– [ä¸­æ–‡æ–‡æ¡£](README_zh.md)**

## ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**: ä½¿ç”¨Goè¯­è¨€æ„å»ºï¼Œå…·æœ‰æœ€ä½³æ€§èƒ½å’Œå¹¶å‘èƒ½åŠ›
- ğŸ“‹ **ä¼˜å…ˆçº§è°ƒåº¦**: å››çº§ä¼˜å…ˆçº§ç³»ç»Ÿï¼ŒåŒçº§åˆ«å†…é‡‡ç”¨FIFOæ’åº
- ğŸ’¾ **æŒä¹…åŒ–å­˜å‚¨**: åŸºäºBadgerDBçš„å­˜å‚¨ç³»ç»Ÿï¼Œæ”¯æŒå´©æºƒæ¢å¤
- ğŸ”„ **é‡è¯•æœºåˆ¶**: å¯é…ç½®çš„é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿ç®—æ³•
- ğŸ¯ **å¤šä»»åŠ¡ç±»å‹**: æ”¯æŒèŠå¤©ã€ç”Ÿæˆå’ŒåµŒå…¥æ“ä½œ
- ğŸ“Š **å®æ—¶ç›‘æ§**: ä»»åŠ¡çŠ¶æ€è·Ÿè¸ªå’Œé˜Ÿåˆ—ç»Ÿè®¡
- ğŸ–¥ï¸ **CLIæ¥å£**: å‘½ä»¤è¡Œå·¥å…·è¿›è¡Œä»»åŠ¡ç®¡ç†
- ğŸ“š **åº“é›†æˆ**: å¯ä½œä¸ºGoåº“åœ¨åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨
- ğŸŒŠ **æµå¼æ”¯æŒ**: èŠå¤©å’Œç”Ÿæˆä»»åŠ¡çš„å®æ—¶æµå¼è¾“å‡º

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
go get github.com/liliang-cn/ollama-queue
```

### ä½œä¸ºåº“çš„åŸºæœ¬ä½¿ç”¨

```go
package main

import (
    "context"
    "fmt"
    "log"

    "github.com/liliang-cn/ollama-queue/internal/models"
    "github.com/liliang-cn/ollama-queue/pkg/queue"
)

func main() {
    // åˆ›å»ºé˜Ÿåˆ—ç®¡ç†å™¨
    qm, err := queue.NewQueueManagerWithOptions(
        queue.WithOllamaHost("http://localhost:11434"),
        queue.WithMaxWorkers(4),
        queue.WithStoragePath("./data"),
    )
    if err != nil {
        log.Fatal(err)
    }
    defer qm.Close()

    // å¯åŠ¨é˜Ÿåˆ—ç®¡ç†å™¨
    ctx := context.Background()
    if err := qm.Start(ctx); err != nil {
        log.Fatal(err)
    }

    // åˆ›å»ºå¹¶æäº¤èŠå¤©ä»»åŠ¡
    task := queue.NewChatTask("llama2", []models.ChatMessage{
        {Role: "user", Content: "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"},
    }, queue.WithTaskPriority(models.PriorityHigh))

    taskID, err := qm.SubmitTask(task)
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("ä»»åŠ¡å·²æäº¤ï¼ŒID: %s\n", taskID)

    // ä½¿ç”¨å›è°ƒç­‰å¾…å®Œæˆ
    _, err = qm.SubmitTaskWithCallback(task, func(result *models.TaskResult) {
        if result.Success {
            fmt.Printf("ä»»åŠ¡å®ŒæˆæˆåŠŸ: %v\n", result.Data)
        } else {
            fmt.Printf("ä»»åŠ¡å¤±è´¥: %s\n", result.Error)
        }
    })
    if err != nil {
        log.Fatal(err)
    }
}
```

### CLIä½¿ç”¨

```bash
# æäº¤èŠå¤©ä»»åŠ¡
ollama-queue submit chat --model llama2 --messages "user:ä½ å¥½ï¼Œä½ èƒ½å¸®æˆ‘ä»€ä¹ˆï¼Ÿ" --priority high

# æäº¤ç”Ÿæˆä»»åŠ¡
ollama-queue submit generate --model codellama --prompt "ç¼–å†™ä¸€ä¸ªGoå‡½æ•°æ¥æ’åºæ•°ç»„"

# åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡
ollama-queue list

# æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
ollama-queue status

# å–æ¶ˆä»»åŠ¡
ollama-queue cancel <task-id>

# æ›´æ–°ä»»åŠ¡ä¼˜å…ˆçº§
ollama-queue priority <task-id> high
```

## ä»»åŠ¡ç±»å‹

### èŠå¤©ä»»åŠ¡
ç”¨äºä¸è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯äº¤äº’ã€‚

```go
task := queue.NewChatTask("llama2", []models.ChatMessage{
    {Role: "system", Content: "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
    {Role: "user", Content: "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—"},
}, 
    queue.WithTaskPriority(models.PriorityHigh),
    queue.WithChatSystem("ä½ æ˜¯ä¸€ä¸ªç‰©ç†å­¦ä¸“å®¶"),
    queue.WithChatStreaming(true),
)
```

### ç”Ÿæˆä»»åŠ¡
ç”¨äºæ–‡æœ¬ç”Ÿæˆå’Œè¡¥å…¨ã€‚

```go
task := queue.NewGenerateTask("codellama", "ç¼–å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°",
    queue.WithTaskPriority(models.PriorityNormal),
    queue.WithGenerateSystem("ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹"),
    queue.WithGenerateTemplate("### å›å¤:\n{{ .Response }}"),
)
```

### åµŒå…¥ä»»åŠ¡
ç”¨äºåˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡ã€‚

```go
task := queue.NewEmbedTask("nomic-embed-text", "è¿™æ˜¯ç”¨äºåµŒå…¥çš„ç¤ºä¾‹æ–‡æœ¬",
    queue.WithTaskPriority(models.PriorityNormal),
    queue.WithEmbedTruncate(true),
)
```

## ä¼˜å…ˆçº§ç³»ç»Ÿ

ç³»ç»Ÿæ”¯æŒå››ä¸ªä¼˜å…ˆçº§ç­‰çº§ï¼š

- **å…³é”® (15)**: æœ€é«˜ä¼˜å…ˆçº§ï¼Œæœ€å…ˆå¤„ç†
- **é«˜ (10)**: é«˜ä¼˜å…ˆçº§ä»»åŠ¡
- **æ™®é€š (5)**: é»˜è®¤ä¼˜å…ˆçº§ç­‰çº§
- **ä½ (1)**: æœ€ä½ä¼˜å…ˆçº§ä»»åŠ¡

é«˜ä¼˜å…ˆçº§ä»»åŠ¡æ€»æ˜¯åœ¨ä½ä¼˜å…ˆçº§ä»»åŠ¡ä¹‹å‰å¤„ç†ã€‚åœ¨åŒä¸€ä¼˜å…ˆçº§å†…ï¼Œä»»åŠ¡æŒ‰ç…§FIFOé¡ºåºå¤„ç†ã€‚

## é…ç½®

### ä½¿ç”¨é…ç½®æ–‡ä»¶

åˆ›å»º `config.yaml` æ–‡ä»¶ï¼š

```yaml
ollama:
  host: "http://localhost:11434"
  timeout: "5m"

queue:
  max_workers: 4
  storage_path: "./data"
  cleanup_interval: "1h"
  max_completed_tasks: 1000

scheduler:
  batch_size: 10
  scheduling_interval: "1s"

retry_config:
  max_retries: 3
  initial_delay: "1s"
  max_delay: "30s"
  backoff_factor: 2.0

logging:
  level: "info"
  file: "ollama-queue.log"
```

### ç¯å¢ƒå˜é‡

```bash
export OLLAMA_HOST="http://localhost:11434"
export QUEUE_MAX_WORKERS=4
export QUEUE_STORAGE_PATH="./data"
export RETRY_MAX_RETRIES=3
export LOG_LEVEL="info"
```

## åº“é›†æˆ

### åœ¨Webåº”ç”¨ä¸­ä½¿ç”¨

å°†Ollama Queueé›†æˆåˆ°æ‚¨çš„WebæœåŠ¡ä¸­ï¼Œå®ç°é«˜æ•ˆçš„AIä»»åŠ¡å¤„ç†ï¼š

```go
package main

import (
    "context"
    "encoding/json"
    "net/http"
    "log"

    "github.com/gin-gonic/gin"
    "github.com/liliang-cn/ollama-queue/internal/models"
    "github.com/liliang-cn/ollama-queue/pkg/queue"
)

type ChatRequest struct {
    Message string `json:"message"`
    Model   string `json:"model"`
}

var queueManager *queue.QueueManager

func main() {
    // åˆå§‹åŒ–é˜Ÿåˆ—ç®¡ç†å™¨
    var err error
    queueManager, err = queue.NewQueueManagerWithOptions(
        queue.WithMaxWorkers(8),
        queue.WithStoragePath("./web_queue_data"),
    )
    if err != nil {
        log.Fatal(err)
    }
    defer queueManager.Close()

    // å¯åŠ¨é˜Ÿåˆ—ç®¡ç†å™¨
    ctx := context.Background()
    if err := queueManager.Start(ctx); err != nil {
        log.Fatal(err)
    }

    r := gin.Default()
    r.POST("/chat", handleChat)
    r.GET("/task/:id", handleTaskStatus)
    r.GET("/queue/stats", handleQueueStats)
    
    r.Run(":8080")
}

func handleChat(c *gin.Context) {
    var req ChatRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(400, gin.H{"error": err.Error()})
        return
    }

    task := queue.NewChatTask(req.Model, []models.ChatMessage{
        {Role: "user", Content: req.Message},
    })

    taskID, err := queueManager.SubmitTask(task)
    if err != nil {
        c.JSON(500, gin.H{"error": err.Error()})
        return
    }

    c.JSON(200, gin.H{
        "task_id": taskID,
        "status": "submitted",
    })
}

func handleTaskStatus(c *gin.Context) {
    taskID := c.Param("id")
    
    task, err := queueManager.GetTask(taskID)
    if err != nil {
        c.JSON(404, gin.H{"error": "ä»»åŠ¡æœªæ‰¾åˆ°"})
        return
    }

    c.JSON(200, gin.H{
        "task_id": task.ID,
        "status": task.Status,
        "result": task.Result,
        "error": task.Error,
    })
}
```

### å¼‚æ­¥ä»»åŠ¡å¤„ç†

åˆ›å»ºä»»åŠ¡å¤„ç†å™¨æ¥å¤„ç†åå°æ“ä½œï¼š

```go
package main

import (
    "context"
    "fmt"
    "sync"
    "time"

    "github.com/liliang-cn/ollama-queue/internal/models"
    "github.com/liliang-cn/ollama-queue/pkg/queue"
)

type TaskProcessor struct {
    qm *queue.QueueManager
    wg sync.WaitGroup
}

func NewTaskProcessor() *TaskProcessor {
    qm, err := queue.NewQueueManagerWithOptions(
        queue.WithMaxWorkers(6),
        queue.WithStoragePath("./processor_data"),
    )
    if err != nil {
        panic(err)
    }

    processor := &TaskProcessor{qm: qm}
    
    ctx := context.Background()
    if err := qm.Start(ctx); err != nil {
        panic(err)
    }

    return processor
}

func (tp *TaskProcessor) ProcessChatAsync(message, model string, callback func(string, error)) {
    task := queue.NewChatTask(model, []models.ChatMessage{
        {Role: "user", Content: message},
    })

    tp.wg.Add(1)
    _, err := tp.qm.SubmitTaskWithCallback(task, func(result *models.TaskResult) {
        defer tp.wg.Done()
        
        if result.Success {
            callback(fmt.Sprintf("%v", result.Data), nil)
        } else {
            callback("", fmt.Errorf(result.Error))
        }
    })

    if err != nil {
        tp.wg.Done()
        callback("", err)
    }
}

func (tp *TaskProcessor) ProcessBatch(messages []string, model string) ([]string, error) {
    var tasks []*models.Task
    
    for _, msg := range messages {
        task := queue.NewChatTask(model, []models.ChatMessage{
            {Role: "user", Content: msg},
        })
        tasks = append(tasks, task)
    }

    results, err := tp.qm.SubmitBatchTasks(tasks)
    if err != nil {
        return nil, err
    }

    var responses []string
    for _, result := range results {
        if result.Success {
            responses = append(responses, fmt.Sprintf("%v", result.Data))
        } else {
            responses = append(responses, fmt.Sprintf("é”™è¯¯: %s", result.Error))
        }
    }

    return responses, nil
}

func (tp *TaskProcessor) Close() error {
    tp.wg.Wait()
    return tp.qm.Close()
}
```

### é…ç½®é€‰é¡¹

ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´é…ç½®è®¾ç½®ï¼š

```go
// å®Œæ•´é…ç½®ç¤ºä¾‹
qm, err := queue.NewQueueManagerWithOptions(
    // Ollama é…ç½®
    queue.WithOllamaHost("http://localhost:11434"),
    queue.WithOllamaTimeout(5*time.Minute),
    
    // é˜Ÿåˆ—é…ç½®
    queue.WithMaxWorkers(8),
    queue.WithStoragePath("./my_app_queue"),
    queue.WithCleanupInterval(time.Hour),
    queue.WithMaxCompletedTasks(1000),
    queue.WithSchedulingInterval(time.Second),
    
    // é‡è¯•é…ç½®
    queue.WithRetryConfig(models.RetryConfig{
        MaxRetries:    3,
        InitialDelay:  time.Second,
        MaxDelay:      30 * time.Second,
        BackoffFactor: 2.0,
    }),
    
    // æ—¥å¿—é…ç½®
    queue.WithLogLevel("info"),
    queue.WithLogFile("./logs/queue.log"),
)
```

## é«˜çº§ç”¨æ³•

### æµå¼ä»»åŠ¡

```go
// æäº¤æµå¼ä»»åŠ¡
streamChan, err := qm.SubmitStreamingTask(task)
if err != nil {
    log.Fatal(err)
}

// å¤„ç†æµå¼è¾“å‡º
for chunk := range streamChan {
    if chunk.Error != nil {
        fmt.Printf("æµå¼é”™è¯¯: %v\n", chunk.Error)
        break
    }
    
    if chunk.Done {
        fmt.Println("\næµå¼å®Œæˆ")
        break
    }
    
    fmt.Print(chunk.Data)
}
```

### æ‰¹é‡å¤„ç†

```go
// æäº¤å¤šä¸ªä»»åŠ¡
tasks := []*models.Task{
    queue.NewChatTask("llama2", []models.ChatMessage{...}),
    queue.NewGenerateTask("codellama", "ç¼–å†™ä¸€ä¸ªå‡½æ•°"),
    queue.NewEmbedTask("nomic-embed-text", "ç¤ºä¾‹æ–‡æœ¬"),
}

// åŒæ­¥æ‰¹é‡å¤„ç†
results, err := qm.SubmitBatchTasks(tasks)
if err != nil {
    log.Fatal(err)
}

// å¤„ç†ç»“æœ
for i, result := range results {
    fmt.Printf("ä»»åŠ¡ %d: æˆåŠŸ=%v, æ•°æ®=%v\n", i, result.Success, result.Data)
}
```

### äº‹ä»¶ç›‘æ§

```go
// è®¢é˜…ä»»åŠ¡äº‹ä»¶
eventChan, err := qm.Subscribe([]string{"task_completed", "task_failed"})
if err != nil {
    log.Fatal(err)
}

// ç›‘æ§äº‹ä»¶
go func() {
    for event := range eventChan {
        fmt.Printf("äº‹ä»¶: %s, ä»»åŠ¡: %s, çŠ¶æ€: %s\n", 
            event.Type, event.TaskID, event.Status)
    }
}()
```

## APIå‚è€ƒ

### QueueManageræ¥å£

```go
type QueueManagerInterface interface {
    // ç”Ÿå‘½å‘¨æœŸç®¡ç†
    Start(ctx context.Context) error
    Stop() error
    Close() error

    // ä»»åŠ¡æäº¤
    SubmitTask(task *models.Task) (string, error)
    SubmitTaskWithCallback(task *models.Task, callback models.TaskCallback) (string, error)
    SubmitTaskWithChannel(task *models.Task, resultChan chan *models.TaskResult) (string, error)
    SubmitStreamingTask(task *models.Task) (<-chan *models.StreamChunk, error)

    // ä»»åŠ¡ç®¡ç†
    GetTask(taskID string) (*models.Task, error)
    GetTaskStatus(taskID string) (models.TaskStatus, error)
    CancelTask(taskID string) error
    UpdateTaskPriority(taskID string, priority models.Priority) error

    // æŸ¥è¯¢æ“ä½œ
    ListTasks(filter models.TaskFilter) ([]*models.Task, error)
    GetQueueStats() (*models.QueueStats, error)

    // äº‹ä»¶ç›‘æ§
    Subscribe(eventTypes []string) (<-chan *models.TaskEvent, error)
    Unsubscribe(eventChan <-chan *models.TaskEvent) error
}
```

## CLIå‘½ä»¤

| å‘½ä»¤ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| `submit` | æäº¤æ–°ä»»åŠ¡ | `ollama-queue submit chat --model llama2 --messages "user:ä½ å¥½"` |
| `list` | åˆ—å‡ºä»»åŠ¡ï¼ˆæ”¯æŒè¿‡æ»¤ï¼‰ | `ollama-queue list --status running --limit 10` |
| `status` | æ˜¾ç¤ºä»»åŠ¡çŠ¶æ€æˆ–é˜Ÿåˆ—ç»Ÿè®¡ | `ollama-queue status <task-id>` |
| `cancel` | å–æ¶ˆä¸€ä¸ªæˆ–å¤šä¸ªä»»åŠ¡ | `ollama-queue cancel <task-id1> <task-id2>` |
| `priority` | æ›´æ–°ä»»åŠ¡ä¼˜å…ˆçº§ | `ollama-queue priority <task-id> high` |

## ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLIå·¥å…·       â”‚    â”‚   Goåº“          â”‚    â”‚   Web API       â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚   (æœªæ¥)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      é˜Ÿåˆ—ç®¡ç†å™¨             â”‚
                    â”‚                             â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                    â”‚  â”‚ä¼˜å…ˆçº§   â”‚ â”‚   é‡è¯•      â”‚â”‚
                    â”‚  â”‚è°ƒåº¦å™¨   â”‚ â”‚  è°ƒåº¦å™¨     â”‚â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                    â”‚                             â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                    â”‚  â”‚ å­˜å‚¨    â”‚ â”‚  æ‰§è¡Œå™¨     â”‚â”‚
                    â”‚  â”‚(BadgerDB)â”‚ â”‚  (Ollama)   â”‚â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## æ€§èƒ½ç‰¹ç‚¹

- **ååé‡**: æ¯ç§’å¤„ç†æ•°åƒä¸ªä»»åŠ¡
- **å»¶è¿Ÿ**: äºšæ¯«ç§’çº§ä»»åŠ¡è°ƒåº¦
- **å†…å­˜**: é«˜æ•ˆå†…å­˜ä½¿ç”¨ï¼Œå¯é…ç½®é™åˆ¶
- **å­˜å‚¨**: æŒä¹…åŒ–å­˜å‚¨ï¼Œè‡ªåŠ¨æ¸…ç†
- **å¹¶å‘**: å¯é…ç½®å·¥ä½œæ± ï¼Œä¼˜åŒ–èµ„æºåˆ©ç”¨

## é”™è¯¯å¤„ç†

ç³»ç»Ÿæä¾›å…¨é¢çš„é”™è¯¯å¤„ç†ï¼š

- **ä»»åŠ¡å¤±è´¥**: è‡ªåŠ¨é‡è¯•ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿
- **è¿æ¥é”™è¯¯**: ä¼˜é›…é™çº§å’Œæ¢å¤
- **å­˜å‚¨é”™è¯¯**: æ•°æ®ä¸€è‡´æ€§å’ŒæŸåä¿æŠ¤
- **èµ„æºé™åˆ¶**: é˜Ÿåˆ—å¤§å°å’Œå†…å­˜é™åˆ¶ï¼Œæ”¯æŒèƒŒå‹

## è´¡çŒ®

1. Forkæ­¤ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. è¿›è¡Œæ›´æ”¹
4. ä¸ºæ–°åŠŸèƒ½æ·»åŠ æµ‹è¯•
5. è¿è¡Œæµ‹è¯•å¥—ä»¶: `go test ./...`
6. æäº¤Pull Request

## æµ‹è¯•

è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼š
```bash
go test ./...
```

è¿è¡Œæµ‹è¯•å¹¶æŸ¥çœ‹è¦†ç›–ç‡ï¼š
```bash
go test ./... -cover
```

è¿è¡Œç‰¹å®šåŒ…çš„æµ‹è¯•ï¼š
```bash
go test ./pkg/scheduler -v
go test ./pkg/storage -v
go test ./internal/models -v
```

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## æ”¯æŒ

- ğŸ“– æ–‡æ¡£: [docs/](docs/) | [English](README.md)
- ğŸ› é—®é¢˜: [GitHub Issues](https://github.com/liliang-cn/ollama-queue/issues)
- ğŸ’¬ è®¨è®º: [GitHub Discussions](https://github.com/liliang-cn/ollama-queue/discussions)

## ç›¸å…³é¡¹ç›®

- [Ollama](https://github.com/ollama/ollama) - æœ¬åœ°è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹
- [Ollama-Go](https://github.com/liliang-cn/ollama-go) - Ollamaçš„Goå®¢æˆ·ç«¯