# Ollama Queue

ä¸€ä¸ª**é«˜æ€§èƒ½é€šç”¨ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿ**ï¼Œæ”¯æŒå¤šç§æ‰§è¡Œå™¨æ’ä»¶ï¼ŒåŒ…æ‹¬Ollamaæ¨¡å‹ã€OpenAI APIã€è„šæœ¬æ‰§è¡Œç­‰ã€‚

**ğŸ“– [ä¸­æ–‡æ–‡æ¡£](README_zh.md)** | **ğŸŒŸ [English](README.md)**

## âœ¨ ç‰¹æ€§

- ğŸš€ **é€šç”¨æ¶æ„**: æ”¯æŒå¤šç§æ‰§è¡Œå™¨ç±»å‹ (Ollama, OpenAI, Scriptç­‰)
- ğŸ—ï¸ **å®¢æˆ·ç«¯-æœåŠ¡å™¨åˆ†ç¦»**: ç‹¬ç«‹çš„serverå’ŒclientäºŒè¿›åˆ¶æ–‡ä»¶
- ğŸŒ **å®æ—¶Web UI**: æµè§ˆå™¨ç®¡ç†ç•Œé¢
- â° **Cronè°ƒåº¦**: Unixé£æ ¼å®šæ—¶ä»»åŠ¡
- ğŸ“‹ **ä¼˜å…ˆçº§è°ƒåº¦**: å››çº§ä¼˜å…ˆçº§ (1, 5, 10, 15)
- ğŸ’¾ **æŒä¹…åŒ–å­˜å‚¨**: BadgerDBå´©æºƒæ¢å¤
- ğŸ“Š **å®æ—¶ç›‘æ§**: ä»»åŠ¡çŠ¶æ€è¿½è¸ª
- ğŸ“š **é€šç”¨æ¶æ„**: å¯ä½œä¸ºGoåº“ä½¿ç”¨

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…
```bash
go get github.com/liliang-cn/ollama-queue
```

### æœåŠ¡å™¨æ¨¡å¼
```bash
# å¯åŠ¨æœåŠ¡å™¨ (é»˜è®¤ç«¯å£7125)
ollama-queue-server

# è®¿é—®Webç•Œé¢
open http://localhost:7125
```

### å®¢æˆ·ç«¯ä½¿ç”¨
```bash
# æäº¤Ollamaä»»åŠ¡
ollama-queue submit chat --model qwen3 --messages "user:Hello!"

# æäº¤è„šæœ¬ä»»åŠ¡ (æ–°åŠŸèƒ½)
ollama-queue submit generic --executor script --action execute --command "python script.py"

# å®šæ—¶ä»»åŠ¡ (æ”¯æŒæ‰€æœ‰æ‰§è¡Œå™¨)
ollama-queue cron add --name "Daily Report" --schedule "0 9 * * 1-5" --executor ollama --action generate --model qwen3 --prompt "Generate daily business report"

# è„šæœ¬å®šæ—¶ä»»åŠ¡
ollama-queue cron add --name "Backup" --schedule "0 2 * * *" --executor script --action execute --command "python backup.py"

# OpenAIå®šæ—¶ä»»åŠ¡  
ollama-queue cron add --name "Analysis" --schedule "0 */6 * * *" --executor openai --action chat --model gpt-4 --prompt "Analyze system metrics"

# æŸ¥çœ‹é˜Ÿåˆ—çŠ¶æ€
ollama-queue status
```

## ğŸ“š ä½œä¸ºåº“ä½¿ç”¨

### HTTPå®¢æˆ·ç«¯æ¨¡å¼ (æ¨è)
```go
import (
    "github.com/liliang-cn/ollama-queue/pkg/client"
    "github.com/liliang-cn/ollama-queue/pkg/models"
)

// è¿æ¥åˆ°è¿è¡Œä¸­çš„æœåŠ¡å™¨
cli := client.New("localhost:7125")

// æäº¤ä»»åŠ¡åˆ°Ollama
task := models.CreateOllamaTask(
    models.ActionChat,
    "qwen3", 
    map[string]interface{}{
        "messages": []models.ChatMessage{
            {Role: "user", Content: "Hello!"},
        },
    },
)
taskID, _ := cli.SubmitGenericTask(task)

// æäº¤è„šæœ¬æ‰§è¡Œä»»åŠ¡
scriptTask := models.CreateGenericTask(
    models.ExecutorTypeScript,
    models.ActionExecute,
    map[string]interface{}{
        "command": "python analyze.py",
    },
)
cli.SubmitGenericTask(scriptTask)
```

### åµŒå…¥å¼ä½¿ç”¨
```go
import "github.com/liliang-cn/ollama-queue/pkg/executor"

// åˆ›å»ºæ‰§è¡Œå™¨æ³¨å†Œè¡¨
registry := executor.NewExecutorRegistry()

// æ³¨å†Œè‡ªå®šä¹‰æ‰§è¡Œå™¨
scriptPlugin, _ := executor.NewScriptPlugin(executor.ScriptConfig{
    WorkingDir: "./scripts",
})
registry.Register(models.ExecutorTypeScript, scriptPlugin)

// æäº¤å’Œæ‰§è¡Œä»»åŠ¡
task := models.CreateGenericTask(models.ExecutorTypeScript, models.ActionExecute, payload)
result, _ := registry.ExecuteTask(ctx, task)
```

## ğŸ¯ æ”¯æŒçš„æ‰§è¡Œå™¨

| æ‰§è¡Œå™¨ | åŠ¨ä½œ | æè¿° |
|-------|------|------|
| `ollama` | `chat`, `generate`, `embed` | æœ¬åœ°Ollamaæ¨¡å‹ |
| `openai` | `chat`, `generate` | OpenAI APIå…¼å®¹æœåŠ¡ |
| `script` | `execute` | æ‰§è¡Œè„šæœ¬å’Œå‘½ä»¤ |

## ğŸ“ ç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•è·å–å®Œæ•´ç¤ºä¾‹ï¼š
- `examples/universal_queue/` - é€šç”¨é˜Ÿåˆ—åº“ä½¿ç”¨ç¤ºä¾‹
- `examples/basic/` - åŸºæœ¬ä»»åŠ¡æäº¤
- `examples/cron/` - å®šæ—¶ä»»åŠ¡ç¤ºä¾‹

## ğŸ§ª æµ‹è¯•

```bash
go test ./...
go run examples/universal_queue/main.go
```

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

**ollama-queue**: é€šç”¨ä»»åŠ¡è°ƒåº¦å¹³å°ï¼Œæ”¯æŒå¤šç§æ‰§è¡Œå™¨æ’ä»¶ ğŸš€